{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version is : 1.11.0+cpu\n",
      "numpy version is : 1.21.5\n",
      "torchvision version is : 0.12.0+cpu\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from __future__ import division\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "print(f'torch version is : {torch.__version__}')\n",
    "print(f'numpy version is : {np.__version__}')\n",
    "print(f'torchvision version is : {torchvision.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device configration \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to load images\n",
    "def load_images(image_path, transform=None, max_size=None, shape=None):\n",
    "    \"\"\"\n",
    "    Load Images : \n",
    "    \"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    if max_size:\n",
    "        scale = max_size/ max(image.size)\n",
    "        size = np.array(image.size) * scale\n",
    "        image = image.resize(size.astype(int), Image.ANTIALIAS)\n",
    "    if shape:\n",
    "        image = image.resize(shape, Image.LANCZOS)\n",
    "    if transform:\n",
    "        image = transform(image).unsqueeze(0)\n",
    "    return image.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's deifne the model \n",
    "from turtle import forward\n",
    "\n",
    "\n",
    "class VGGNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGNet, self).__init__()\n",
    "        self.select = ['0', '5', '10', '19', '28']\n",
    "        self.vgg = models.vgg19(pretrained=True).features\n",
    "    \n",
    "    # Define the forward \n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        for name, layer in self.vgg.__module.items():\n",
    "            x = layer(x)\n",
    "            if name in self.select:\n",
    "                features.append(x)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config):\n",
    "    \n",
    "    # Image preprocessing\n",
    "    # VGGNet was trained on ImageNet where images are normalized by mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225].\n",
    "    # We use the same normalization statistics here.\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "                             std=(0.229, 0.224, 0.225))])\n",
    "    \n",
    "    # Load content and style images\n",
    "    # Make the style image same size as the content image\n",
    "    content = load_image(config.content, transform, max_size=config.max_size)\n",
    "    style = load_image(config.style, transform, shape=[content.size(2), content.size(3)])\n",
    "    \n",
    "    # Initialize a target image with the content image\n",
    "    target = content.clone().requires_grad_(True)\n",
    "    \n",
    "    optimizer = torch.optim.Adam([target], lr=config.lr, betas=[0.5, 0.999])\n",
    "    vgg = VGGNet().to(device).eval()\n",
    "    \n",
    "    for step in range(config.total_step):\n",
    "        \n",
    "        # Extract multiple(5) conv feature vectors\n",
    "        target_features = vgg(target)\n",
    "        content_features = vgg(content)\n",
    "        style_features = vgg(style)\n",
    "\n",
    "        style_loss = 0\n",
    "        content_loss = 0\n",
    "        for f1, f2, f3 in zip(target_features, content_features, style_features):\n",
    "            # Compute content loss with target and content images\n",
    "            content_loss += torch.mean((f1 - f2)**2)\n",
    "\n",
    "            # Reshape convolutional feature maps\n",
    "            _, c, h, w = f1.size()\n",
    "            f1 = f1.view(c, h * w)\n",
    "            f3 = f3.view(c, h * w)\n",
    "\n",
    "            # Compute gram matrix\n",
    "            f1 = torch.mm(f1, f1.t())\n",
    "            f3 = torch.mm(f3, f3.t())\n",
    "\n",
    "            # Compute style loss with target and style images\n",
    "            style_loss += torch.mean((f1 - f3)**2) / (c * h * w) \n",
    "        \n",
    "        # Compute total loss, backprop and optimize\n",
    "        loss = content_loss + config.style_weight * style_loss \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (step+1) % config.log_step == 0:\n",
    "            print ('Step [{}/{}], Content Loss: {:.4f}, Style Loss: {:.4f}' \n",
    "                   .format(step+1, config.total_step, content_loss.item(), style_loss.item()))\n",
    "\n",
    "        if (step+1) % config.sample_step == 0:\n",
    "            # Save the generated image\n",
    "            denorm = transforms.Normalize((-2.12, -2.04, -1.80), (4.37, 4.46, 4.44))\n",
    "            img = target.clone().squeeze()\n",
    "            img = denorm(img).clamp_(0, 1)\n",
    "            torchvision.utils.save_image(img, 'output-{}.png'.format(step+1))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--content', type=str, default='png/content.png')\n",
    "    parser.add_argument('--style', type=str, default='png/style.png')\n",
    "    parser.add_argument('--max_size', type=int, default=400)\n",
    "    parser.add_argument('--total_step', type=int, default=2000)\n",
    "    parser.add_argument('--log_step', type=int, default=10)\n",
    "    parser.add_argument('--sample_step', type=int, default=500)\n",
    "    parser.add_argument('--style_weight', type=float, default=100)\n",
    "    parser.add_argument('--lr', type=float, default=0.003)\n",
    "    config = parser.parse_args()\n",
    "    print(config)\n",
    "    main(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3865130ac3a4a180d9f67584570fac47f03c39ebaa170ceee2ff80d68fa62cc1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
